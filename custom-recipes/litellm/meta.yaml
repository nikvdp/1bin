{% set name = "litellm" %}
{% set version = "1.12.0" %}

package:
  name: "{{ name|lower }}"
  version: "{{ version }}"

source:
  url: "https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz"
  # sha256: 5c9675e3963962abc2b3c3c1f32cb210921f1e350d0cc3eddc85407ff9964bf5
  sha256: cc54f547ca5e480986416203f1f8c4d0d386eaf7c3f4bea6dfa72856827d0c9e

build:
  number: 0
  entry_points:
    - litellm=litellm:run_server
  script: |
    {{ PYTHON }} -m pip install . -vv
    # {{ PYTHON }} -m pip install --ignore-installed async_generator -vv
    # {{ PYTHON }} -m pip install https://files.pythonhosted.org/packages/71/52/39d20e03abd0ac9159c162ec24b93fbcaa111e8400308f2465432495ca2b/async_generator-1.10-py3-none-any.whl
    # {{ PYTHON }} -m pip install --force https://files.pythonhosted.org/packages/85/4f/d010eca6914703d8e6be222165d02c3e708ed909cdb2b7af3743667f302e/anyio-4.1.0-py3-none-any.whl
    # {{ PYTHON }} -m pip install async_generator

requirements:
  host:
    - pip
    - python >=3.10.12
    - poetry
  run:
    - python >=3.10
    - openai >=1.0.0
    - python-dotenv >=0.2.0
    - tiktoken >=0.4.0
    - importlib-metadata >=6.8.0
    - tokenizers=0.15
    - click=8.1
    - jinja2 >=3.1.2
    - certifi >=2023.7.22
    - appdirs >=1.4.4
    - aiohttp=3.9      
    # below are hidden deps that llm tries to conditionally install in
    # proxy_server.py (grr, thanks litellm)
    # conda doesn't update channel's deps if an older version had different
    # deps, so when testing this locally it's better to do `conda build` with
    # `--output-folder` to save it somewhere new , and *then* use -c to add
    # that folder as a channel when you do the conda install. shouldn't be an 
    # issue in CI since that'll be on a fresh instance
    - uvicorn=0.24
    - fastapi=0.99
    - appdirs=1.4
    - backoff=2.2
    - pyyaml=6.0
    - rq=1.15
    - anyio=4.1
    - orjson=3.9
    - async_generator=1.10

about:
  home: "https://github.com/BerriAI/litellm"
  license: MIT
  license_family: MIT
  license_file: 
  summary: "Call all LLM APIs using the OpenAI format. Use Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs)"
  doc_url: 
  dev_url: 

extra:
  recipe-maintainers:
    - nikvdp
